# acuan https://github.com/yyc9268/Numerical_optimization/blob/49639aa66b0f1dd47803110ae47b1083e4217b51/matlab/multivariate_smooth/quasi_newton.m# acuan paper http://people.math.sfu.ca/~elushi/project_833.pdffunction [Solution, iter, is_converge, norm_grad]=quasi_newton_method(f, X, max_iter, tol)  iter = 0;  n = length(X);  B = eye(n);  is_converge = 0;  while iter < max_iter    iter += 1;        g = find_grad(f, X);        # Cek sudah konvergen apa belum    if abs(g) <= tol      Solution = X';      is_converge = 1;      norm_grad = norm(g);      break    end              # Berbeda dengan yang di buku    H = inv(B);    p = -H*g;        # Begin line search    alpha = generic_line_search(f, X, p, 0);        # iterasi x    X_old = X;    s = alpha*p;    X = X + s;        # Hitung SR-1    g_new = find_grad(f, X);    y_k = g_new - g;    B_old = B;    divisor = (y_k - B*s)'*s;    B = B + (y_k - B*s)*(y_k - B*s)'/divisor;        # Skip update if divisor is close to zero    if abs(divisor) <= 1e-8 || abs(divisor) < 0.5*norm(s)*norm((y_k - B*s))      B = B_old;    end        if iter == max_iter      Solution = X';      norm_grad = norm(g_new);    end      end end
