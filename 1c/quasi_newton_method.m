# acuan https://github.com/yyc9268/Numerical_optimization/blob/49639aa66b0f1dd47803110ae47b1083e4217b51/matlab/multivariate_smooth/quasi_newton.m# acuan paper http://people.math.sfu.ca/~elushi/project_833.pdffunction [Solution, iter, is_converge, norm_grad]=quasi_newton_method(f, X, max_iter, tol)  iter = 0;  n = length(X);  B = eye(n);  is_converge = 0;  while iter < max_iter    iter += 1;        g = find_grad(f, X);        # Cek sudah konvergen apa belum    if abs(g) <= tol      Solution = X';      is_converge = 1;      norm_grad = norm(g);      break    end          # cara di paper http://people.math.sfu.ca/~elushi/project_833.pdf    # H = inv(B);    # p = -H*g;        p = -B*g;        # Begin line search    alpha = generic_line_search(f, X, p, 0);        # iterasi x    # cara di paper    # s = alpha*p;    X_old = X;    X = X + alpha*p;        # Hitung SR-1    g_new = find_grad(f, X);    y_k = g_new - g;    d_k = X - X_old;    B_old = B;        # cara di paper    # divisor = (y_k - B*s)'*s;    divisor = (d_k - B*y_k)'*y_k;    # Skip update if divisor is close to zero    if abs(divisor) <= 1e-8      # cara di paper      # B = B + (y_k - B*s)*(y_k - B*s)'./divisor;      B = B_old;    else      B = B + (d_k - B*y_k)*(d_k - B*y_k)'/divisor;    end        if iter == max_iter      Solution = X';      norm_grad = norm(g_new);    end      end end
